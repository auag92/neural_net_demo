{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>PSP Linkages using Deep Learning</center></h1>\n",
    "<h3><center>Apaar Shanker</center></h3>\n",
    "<h3><center>Georgia Institute of Technology</center></h3>\n",
    " \n",
    "![image.png](pres_imgs/intro_slide.png)\n",
    "\n",
    "* image: Ahmet Cecen et al., Acta Materialia 146 (2018) 76e84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Overview</center></h1>\n",
    "\n",
    "### * Description of Neural Network Model\n",
    "### * Training a Neural Network Model: Backpropogation\n",
    "### * Applications of Neural Net models in materials domain\n",
    "### * Popular Libraries : How to NN?\n",
    "### * Convolutional Neural Networks\n",
    "### * Analogy between CNNs and MKS Localization\n",
    "### * PDE-NETs and learning Differential Equations using Conv-Net filters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>The inevitable brain analogy and the Perceptron</center></h1>\n",
    "![StanfordLectureNotes](pres_imgs/neuron_and_perceptron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Zooming in on the Perceptron</center></h1>\n",
    "\n",
    "![StanfordLectureNotes](pres_imgs/perceptron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>The neural network model</center></h1>\n",
    "\n",
    "## A simple linear model\n",
    " * $f = Wx$\n",
    "     * $x=\\{x_1, x_2,\\cdots,x_n\\}$\n",
    "     * $ W = \n",
    "     \\begin{pmatrix}w_{11} & w_{12} & \\cdots & w_{1n} \\\\w_{21} & w_{22} & \\cdots & w_{2n} \\\\ \\vdots& \\vdots & \\ddots & \\vdots \\\\ a_{m1} & w_{m2} & \\cdots & w_{mn}    \n",
    "     \\end{pmatrix}$\n",
    "     * $f = \\{f_1, f_2,\\cdots,f_m\\}$\n",
    "\n",
    "$ x$ : $(n \\times 1)$, $W_1$ : $(m1 \\times n)$, $f$ : $(m1 \\times 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>The neural network model</center></h1>\n",
    "\n",
    "A simple linear model\n",
    " * $f = Wx$\n",
    " \n",
    "A 2-layer Neural Network\n",
    " * $f = W_2 max(0, W_1x)$\n",
    " \n",
    "$ max(0,x)$ is known as the ReLU (Regularized Linear Unit) function\n",
    "\n",
    "$x$ : ($n \\times 1$), $W_1$ : ($m1 \\times n$), $W_2$ : ($m2 \\times m1$), $f$ : ($m2 \\times 1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>The neural network model</center></h1>\n",
    "\n",
    "A simple linear model\n",
    " * $f = Wx$\n",
    " \n",
    "A 2-layer Neural Network\n",
    " * $f = W_2 max(0, W_1x)$\n",
    "\n",
    "or A 3-layer Neural Network\n",
    " * $f = W_3max(0, W_2 max(0, W_1x))$\n",
    "\n",
    "$x$: ($n \\times 1$), $W_1$: ($m1 \\times n$), $W_2$: ($m2 \\times m1$), $W_3$: ($m3 \\times m2$), $f$: ($m3 \\times 1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>The neural network model</center></h1>\n",
    "\n",
    "A simple linear model\n",
    " * $f = Wx$\n",
    " \n",
    "A 2-layer Neural Network\n",
    " * $f = W_2 max(0, W_1x)$\n",
    "\n",
    "or if you fancy, 3-layer network with both ReLU and Sigmoid activation\n",
    " * $f = W_3max(0, W_2 \\sigma(W_1x))$ where $\\sigma(h) = \\dfrac{1}{1 - \\exp{-h}}$\n",
    "\n",
    "$x$ : ($n \\times 1$), $W_1$ : ($m1 \\times n$), $W_2$ : ($m2 \\times m1$), $W_3$ : ($m3 \\times m2$), $f$ : ($m3 \\times 1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Commonly Used Activation Functions</center></h1>\n",
    "![StanfordLectureNotes](pres_imgs/activation_functions.png)\n",
    "\n",
    "* Sigmoid and tanh functions is most commonly used in MultiLayer Perceptron models, whereas ReLU is the standard for conv-nets described later. \n",
    "* Please note that the derivatives of all these functions are really easy to compute, for eg: $\\dfrac{d \\sigma(x)}{d(x)} = \\sigma(x)(1-\\sigma(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>The Neural Network as a Computational Graph</center></h1>\n",
    "![StanfordLectureNotes](pres_imgs/graph_representation.png)\n",
    "\n",
    "* s denotes the number of nodes(perceptrons) in each layer\n",
    "* This is a fully connected multi-layered perceptron model\n",
    "* Implemented in scikit-learn as the MLPC model and also available in the MATLAB machine learning toolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Training the model: Optimizing the loss function</center></h1>\n",
    "\n",
    "Consider the linear regression model:\n",
    " - $y = w^Tx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h1><center>Training the model: Optimizing the loss function</center></h1>\n",
    "\n",
    "Consider the linear model:\n",
    " - $y = w^Tx$\n",
    "\n",
    "We can define a function $\\mathcal{L}$:\n",
    "\\begin{align}\n",
    "    \\mathcal{L(w)} &= \\sum^{N}(\\hat{y_i} - y_i)^2\\\\\n",
    "    &= \\sum^{N}(\\hat{y_i} - w^Tx_i)^2\n",
    "\\end{align}\n",
    "such that the problem of guessing the weights reduces to the problem of minimizing the function $\\mathcal{L}$ also known as the loss function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h1><center>Training the model: Optimizing the loss function</center></h1>\n",
    "\n",
    "Consider the linear model:\n",
    " - $y = w^Tx$\n",
    "\n",
    "We can define a function $\\mathcal{L}$:\n",
    "\\begin{align}\n",
    "    \\mathcal{L(w)} &= \\sum^{N}(\\hat{y_i} - y_i)^2\\\\\n",
    "    &= \\sum^{N}(\\hat{y_i} - w^Tx_i)^2\n",
    "\\end{align}\n",
    "such that the problem of guessing the weights reduces to the problem of minimizing the function $\\mathcal{L}$ also known as the loss function.\n",
    "\n",
    "**In this case, the function $\\mathcal{L}$ is clearly convex, i.e. a parabola in $w$ space, so we have an analytical solution to the problem as:**\n",
    " - $\\hat{w} = (X^TX)^{-1}X^T\\hat{Y}$ where $X: \\{x_i\\}$ and $\\hat{Y}: \\{\\hat{y}_i\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Training the model: Gradient Descent</center></h1>\n",
    "\n",
    "![StanfordLectureNotes](pres_imgs/mountains.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Training the model: Gradient Descent</center></h1>\n",
    "\n",
    "* The gradient at any point in the loss function denoted as *$\\nabla_w\\mathcal{L}$*   \n",
    "  \n",
    "* It is a vector that gives the direction of maximal positive change in the loss function.  \n",
    "  \n",
    "* As such, loss function can be minimized by moving in the direction opposite to the gradient.  \n",
    "  \n",
    "* This gives us an update rule  \n",
    "    * $w_{i}^{t+1} = w_{i}^{t} - \\lambda \\dfrac{\\partial \\mathcal{L(w)}}{\\partial{w_i}}$\n",
    "    * $\\lambda$ is reffered to as the learning rate and controls the speed of descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![StanfordLectureNotes](pres_imgs/grad_descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Training the model: Stochastic Gradient Descent</center></h1>\n",
    "\n",
    "\n",
    "* Recal:\n",
    "    * $\\mathcal{L} = \\dfrac{1}{N}\\sum^{N}(\\hat{y}_i - f(x_i))^{2}$\n",
    "* For large datasets, it is expensive to compute loss for the entire dataset in each update step.\n",
    "* An alternative is to compute gradient over batches of training data.\n",
    "* **Stochastic refers to the fact that the \"mini-batch\" loss function is a \"stochastic\" approximation of the actual loss**\n",
    "* This gives us a modified update rule  \n",
    "    * $w_{i}^{t+1} = w_{i}^{t} - \\lambda \\dfrac{\\partial \\mathcal{l_j(w)}}{\\partial{w_i}}$\n",
    "    * $\\lambda$ is reffered to as the learning rate and controls the speed of descent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Training the model: Backpropogation</center></h1>\n",
    "\n",
    "\n",
    "* Recal the form of the 3-layer Neural Network Model:\n",
    "    * $f = W_3max(0, W_2 max(0, W_1x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h1><center>Training the model: Backpropogation</center></h1>\n",
    "\n",
    "\n",
    "* Recal the form of the 3-layer Neural Network Model:\n",
    "    * $f = W_3max(0, W_2 max(0, W_1x))$\n",
    "* We again define the loss function as:\n",
    "    * $\\mathcal{L} = \\dfrac{1}{N}\\sum^{N}(\\hat{y}_i - f(x_i))^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h1><center>Training the model: Backpropogation</center></h1>\n",
    "\n",
    "\n",
    "* Recal the form of the 3-layer Neural Network Model:\n",
    "    * $f = W_3\\sigma(W_2 \\sigma(W_1x))$\n",
    "* We again define the loss function as:\n",
    "    * $\\mathcal{L} = \\dfrac{1}{N}\\sum^{N}(\\hat{y}_i - f(x_i))^{2}$\n",
    "* We would like to use the gradient descent strategy for optimizing $L$ which is:\n",
    "    * $w_{i,j}^{l}[t+1] = w_{i,j}^{l}[t] - \\lambda \\dfrac{\\partial \\mathcal{L(w[t])}}{\\partial{w^l_{i,j}[t]}}$\n",
    "    * where l is the index of the layer and i,j are indices of the parameter in the parameter matrix\n",
    "* However, because of the deep nesting of weights, it is difficult to the get the analytic form of the partial derivative:\n",
    "    * $\\dfrac{\\partial \\mathcal{L(w[t])}}{\\partial{w^l_{i,j}[t]}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Training the model: Backpropogation</center></h1>\n",
    "\n",
    "## What if we use chain rule?\n",
    "\n",
    "Recall, chain rule:  \n",
    "\\begin{align}\n",
    "\\dfrac{d(f\\cdot g)(x)}{dx} = \\dfrac{f(g(x))}{d(g(x))}\\dfrac{d(g(x))}{dx}\n",
    "\\end{align}\n",
    "\n",
    "* A simplified illustration of backpropogation using the univariate logistic least squares model\n",
    "\n",
    "![StanfordLectureNotes](pres_imgs/back_prop_simple.png)\n",
    "\n",
    "http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/slides/lec6.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Training the model: Backpropogation</center></h1>\n",
    "   \n",
    "![StanfordLectureNotes](pres_imgs/back_prop_mlpc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Training the model: Backpropogation</center></h1>\n",
    " \n",
    "  \n",
    "   \n",
    "![StanfordLectureNotes](pres_imgs/back_prop_mlpc_vectorized.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Training the model: Backpropogation</center></h1>\n",
    "\n",
    "* In the message passing notation:  \n",
    "  \n",
    "   \n",
    "![StanfordLectureNotes](pres_imgs/back-prop.png)\n",
    "\n",
    "* For each training step, a forward pass results in the computation of the loss value\n",
    "* A backward pass results in the updation of the weigts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Back to the equation</center></h1>\n",
    "### A 3-layer feed-forward Neural Network\n",
    " * $f = W_3max(0, W_2 max(0, W_1x))$\n",
    " \n",
    "### To Summarize:\n",
    "* A multilayered perceptron is a just a set of linear followed by non-linear transforms performed on a input vector.\n",
    "* A feed-forward fully connected neural network with a single hidden layer using practically any nonlinear activation function can approximate any continuous function of any number of real variables on any compact set to any desired degree of accuracy.\n",
    "* Number of Parameters in the model = $\\sum_{i=1}^{N} (L_{n-1}+1)*L_n$\n",
    "* **How to guess the values of these parameters?**\n",
    "* https://papers.nips.cc/paper/874-how-to-choose-an-activation-function.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Resources for implementing Neural Networks</center></h1>\n",
    "\n",
    " - Pytorch - http://pytorch.org/\n",
    " - Tensorflow - http://tensorflow.org/\n",
    " - Theano - http://deeplearning.net/software/theano/\n",
    " - Keras - https://keras.io/\n",
    " \n",
    " A useful learning resource - \n",
    " https://playground.tensorflow.org/\n",
    " \n",
    " Background\n",
    " http://cs231n.github.io/\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Convolutional Neural Networks</center></h1>\n",
    "\n",
    "![StanfordLectureNotes](pres_imgs/CNN_template.png)\n",
    "\n",
    "Gradient Based Learning applied to document recognition [LeCun, Bottou, Bengio, Haffner 1998]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Convolutional Neural Networks</center></h1>\n",
    "\n",
    "* Image data are high dimensional and have local embedded structures.  \n",
    "\n",
    "* CNNs were conceptualized to overcome the limitations of Fully Connected neural networks in processing image data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Convolutional Neural Networks</center></h1>\n",
    "![StanfordLectureNotes](pres_imgs/fc_layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Convolutional Neural Networks</center></h1>\n",
    "![StanfordLectureNotes](pres_imgs/conv_layer_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall convolution:\n",
    "\n",
    "\\begin{align}\n",
    "f[x, y] * g[x,y] = \\sum_{n_1 = -\\inf}^{\\inf}\\sum_{n_2 = -\\inf}^{\\inf} f[n_1, n_2]\\cdot g[x-n_1, y-n_2]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Convolutional Neural Networks</center></h1>\n",
    "![StanfordLectureNotes](pres_imgs/conv_layer_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Convolutional Neural Networks</center></h1>\n",
    "![StanfordLectureNotes](pres_imgs/conv_layer_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Convolutional Neural Networks</center></h1>\n",
    "![StanfordLectureNotes](pres_imgs/conv_layer_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Convolutional Neural Networks</center></h1>\n",
    "![StanfordLectureNotes](pres_imgs/conv_net_full.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Convolutional Neural Networks</center></h1>\n",
    "\n",
    "## Inorder to reduce number of parameters and prevent overfitting.\n",
    "![StanfordLectureNotes](pres_imgs/pooling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Convolutional Neural Networks</center></h1>\n",
    "\n",
    "## Typical off the shelf CNN / Deep Learning Model\n",
    "### Representation Learning versus Template learning\n",
    "### A compaction of the typical approach of MKS workflow which involves first feature generation followed by linkage\n",
    "\n",
    "![StanfordLectureNotes](pres_imgs/typical_cnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Convolutional Neural Networks</center></h1>\n",
    "\n",
    "## VGG-Net :  A Production CNN\n",
    "\n",
    "![StanfordLectureNotes](pres_imgs/vgg_net.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Convolutional Neural Networks</center></h1>\n",
    "\n",
    "## Why you should care?\n",
    "\n",
    "![StanfordLectureNotes](pres_imgs/image_net.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
